#pragma once

#include <string>
#include <vector>
#include <queue>
#include <memory>
#include <fstream>
#include <sstream>
#include <filesystem>

#include "event_row.h"
#include "csv_reader.h"
#include "arrow_reader.h"
#include "parquet_writer.h"
#include "gz_writer.h"
#include "time_utils.h"
#include "file_utils.h"
#include "logger.h"

/**
 * Stage 2: Merge sorted runs within each shard to produce final output.
 * 
 * This stage:
 * 1. For each shard, finds all run files
 * 2. Performs k-way merge using a priority queue (heap)
 * 3. If too many files, does multi-level merging
 * 4. Writes final sorted output with human-readable timestamps
 */
namespace etl {

/**
 * Reader for run files produced by the partition stage (Arrow IPC format).
 */
class RunFileReader {
public:
    explicit RunFileReader(const std::string& path) 
        : reader_(path), path_(path) {}
    
    /**
     * Read the next event from the run file.
     * Returns false at EOF.
     */
    bool next(EventRow& event) {
        return reader_.next(event);
    }
    
    const std::string& path() const { return path_; }

private:
    ArrowReader reader_;
    std::string path_;
};

/**
 * Heap item for k-way merge.
 * Contains an event and the index of the reader it came from.
 */
struct HeapItem {
    EventRow event;
    size_t reader_index;
    
    /**
     * Comparison for max-heap (std::priority_queue).
     * We invert the comparison to get min-heap behavior.
     */
    bool operator<(const HeapItem& other) const {
        // Invert EventRow comparison for min-heap
        return other.event < event;
    }
};

/**
 * Perform k-way merge of multiple sorted run files and write to Parquet.
 * 
 * @param input_files Paths to sorted run files (Arrow IPC)
 * @param output_file Path to output file
 * @param config Configuration for table/column name lookups
 * @param profile Enable profiling
 */
inline void merge_run_files_to_parquet(const std::vector<std::string>& input_files,
                                        const std::string& output_file,
                                        const Config& config,
                                        bool profile = false) {
    
    if (input_files.empty()) {
        return;
    }
    
    // Profiling timers
    auto profile_start = std::chrono::steady_clock::now();
    int64_t time_opening_files_ms = 0;
    int64_t time_heap_init_ms = 0;
    int64_t time_reading_ms = 0;
    int64_t time_heap_ops_ms = 0;
    int64_t time_writing_ms = 0;
    
    // Open all input files (Arrow IPC format)
    auto t0 = std::chrono::steady_clock::now();
    std::vector<std::unique_ptr<RunFileReader>> readers;
    for (const auto& path : input_files) {
        try {
            readers.emplace_back(std::make_unique<RunFileReader>(path));
        } catch (const std::exception& e) {
            if (!std::filesystem::exists(path)) {
                throw std::runtime_error("File does not exist: " + path);
            }
            throw std::runtime_error("Failed to open run file: " + path + " (" + e.what() + ")");
        }
    }
    auto t1 = std::chrono::steady_clock::now();
    time_opening_files_ms = std::chrono::duration_cast<std::chrono::milliseconds>(t1 - t0).count();
    
    // Open Parquet output file
    ParquetWriter parquet_writer(output_file, &config);
    
    // Initialize heap with first event from each reader
    auto t2 = std::chrono::steady_clock::now();
    std::priority_queue<HeapItem> heap;
    for (size_t i = 0; i < readers.size(); ++i) {
        EventRow event;
        if (readers[i]->next(event)) {
            heap.push(HeapItem{event, i});
        }
    }
    auto t3 = std::chrono::steady_clock::now();
    time_heap_init_ms = std::chrono::duration_cast<std::chrono::milliseconds>(t3 - t2).count();
    
    // K-way merge with batch writing to Parquet
    uint64_t events_written = 0;
    std::vector<EventRow> batch;
    batch.reserve(10000);  // Write in batches of 10K rows
    
    while (!heap.empty()) {
        auto heap_pop_start = std::chrono::steady_clock::now();
        HeapItem item = heap.top();
        heap.pop();
        auto heap_pop_end = std::chrono::steady_clock::now();
        time_heap_ops_ms += std::chrono::duration_cast<std::chrono::milliseconds>(heap_pop_end - heap_pop_start).count();
        
        // Add to batch
        batch.push_back(item.event);
        ++events_written;
        
        // Write batch when full
        if (batch.size() >= 10000) {
            auto write_start = std::chrono::steady_clock::now();
            parquet_writer.write_batch(batch);
            auto write_end = std::chrono::steady_clock::now();
            time_writing_ms += std::chrono::duration_cast<std::chrono::milliseconds>(write_end - write_start).count();
            batch.clear();
        }
        
        // Read next event from the same reader
        auto read_start = std::chrono::steady_clock::now();
        EventRow next_event;
        if (readers[item.reader_index]->next(next_event)) {
            auto read_end = std::chrono::steady_clock::now();
            time_reading_ms += std::chrono::duration_cast<std::chrono::milliseconds>(read_end - read_start).count();
            
            auto heap_push_start = std::chrono::steady_clock::now();
            heap.push(HeapItem{next_event, item.reader_index});
            auto heap_push_end = std::chrono::steady_clock::now();
            time_heap_ops_ms += std::chrono::duration_cast<std::chrono::milliseconds>(heap_push_end - heap_push_start).count();
        }
    }
    
    // Write remaining batch
    if (!batch.empty()) {
        auto write_start = std::chrono::steady_clock::now();
        parquet_writer.write_batch(batch);
        auto write_end = std::chrono::steady_clock::now();
        time_writing_ms += std::chrono::duration_cast<std::chrono::milliseconds>(write_end - write_start).count();
    }
    
    parquet_writer.close();
    
    if (profile) {
        auto profile_end = std::chrono::steady_clock::now();
        int64_t total_ms = std::chrono::duration_cast<std::chrono::milliseconds>(profile_end - profile_start).count();
        
        std::ostringstream profile_msg;
        profile_msg << "  Merge profile for " << input_files.size() << " files (" << events_written << " rows, " << total_ms << "ms):\n"
                    << "    Opening files:   " << time_opening_files_ms << "ms (" 
                    << std::fixed << std::setprecision(1) << (100.0 * time_opening_files_ms / total_ms) << "%)\n"
                    << "    Heap init:       " << time_heap_init_ms << "ms (" 
                    << std::fixed << std::setprecision(1) << (100.0 * time_heap_init_ms / total_ms) << "%)\n"
                    << "    Reading Arrow:   " << time_reading_ms << "ms (" 
                    << std::fixed << std::setprecision(1) << (100.0 * time_reading_ms / total_ms) << "%)\n"
                    << "    Heap ops:        " << time_heap_ops_ms << "ms (" 
                    << std::fixed << std::setprecision(1) << (100.0 * time_heap_ops_ms / total_ms) << "%)\n"
                    << "    Writing Parquet: " << time_writing_ms << "ms (" 
                    << std::fixed << std::setprecision(1) << (100.0 * time_writing_ms / total_ms) << "%)\n"
                    << "    Other/overhead:  " << (total_ms - time_opening_files_ms - time_heap_init_ms - 
                                                    time_reading_ms - time_heap_ops_ms - time_writing_ms) << "ms";
        LOG_INFO(profile_msg.str());
    } else {
        LOG_DEBUG("Merged " + std::to_string(input_files.size()) + " files -> " + 
                  output_file + " (" + std::to_string(events_written) + " events)");
    }
}
}
}

/**
 * Merge all runs for a single shard.
 * 
 * If there are too many run files, performs multi-level merging:
 * - Level 1: Merge groups of max_open files into intermediate files
 * - Level 2: Merge intermediate files
 * - Continue until we have one file
 * 
 * @param temp_root Temporary directory root
 * @param output_root Final output directory root
 * @param shard_id Shard number to merge
 * @param max_open Maximum files to merge at once
 */
inline void merge_shard(const std::filesystem::path& temp_root,
                       const std::filesystem::path& output_root,
                       size_t shard_id,
                       size_t max_open,
                       const Config& config,
                       bool no_gzip = false,
                       bool profile = false) {
    
    namespace fs = std::filesystem;
    
    fs::path shard_dir = temp_root / ("shard=" + std::to_string(shard_id));
    if (!fs::exists(shard_dir)) {
        return;
    }
    
    // Find all run files for this shard (flat structure) - now .arrow files
    std::vector<std::string> run_files;
    for (const auto& entry : fs::directory_iterator(shard_dir)) {
        if (entry.is_regular_file() && entry.path().extension() == ".arrow") {
            run_files.push_back(entry.path().string());
        }
    }
    
    if (run_files.empty()) {
        LOG_WARNING("No run files found for shard " + std::to_string(shard_id));
        return;
    }
    
    LOG_INFO("Merging shard " + std::to_string(shard_id) + 
             " (" + std::to_string(run_files.size()) + " run files)");
    
    // Multi-level merge if needed
    auto merge_level = [&](std::vector<std::string> inputs) -> std::vector<std::string> {
        if (inputs.size() <= max_open) {
            return inputs;
        }
        
        std::vector<std::string> outputs;
        for (size_t i = 0; i < inputs.size(); i += max_open) {
            size_t end = std::min(i + max_open, inputs.size());
            
            std::vector<std::string> batch(inputs.begin() + i, inputs.begin() + end);
            
            fs::path output_path = temp_root / ("shard=" + std::to_string(shard_id)) /
                                   ("merged-" + std::to_string(g_run_sequence++) + ".csv");
            
            merge_run_files(batch, output_path.string(), false, false, nullptr, profile);
            outputs.push_back(output_path.string());
            
            // Delete input files to save space
            for (const auto& input : batch) {
                std::error_code ec;
                fs::remove(input, ec);
            }
        }
        
        return outputs;
    };
    
    // Keep merging until we have <= max_open files
    while (run_files.size() > max_open) {
        run_files = merge_level(run_files);
    }
    
    // Final merge to output directory
    fs::path in_progress_dir = output_root / ("shard=" + std::to_string(shard_id) + ".inprogress");
    std::error_code ec;
    fs::remove_all(in_progress_dir, ec);
    fs::create_directories(in_progress_dir);
    
    fs::path final_output = in_progress_dir / "part-0000.parquet";
    merge_run_files_to_parquet(run_files, final_output.string(), config, profile);
    
    // Atomically rename to final location
    fs::path final_dir = output_root / ("shard=" + std::to_string(shard_id));
    fs::remove_all(final_dir, ec);
    fs::rename(in_progress_dir, final_dir);
    
    LOG_INFO("Completed shard " + std::to_string(shard_id));
}

} // namespace etl

